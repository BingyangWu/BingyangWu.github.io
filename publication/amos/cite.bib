@inproceedings{10.1145/3470496.3527440,
author = {Zheng, Size and Chen, Renze and Wei, Anjiang and Jin, Yicheng and Han, Qin and Lu, Liqiang and Wu, Bingyang and Li, Xiuhong and Yan, Shengen and Liang, Yun},
title = {AMOS: Enabling <U>A</U>Utomatic <U>M</U>Apping for Tensor Computations <U>O</U>n <U>S</U>Patial Accelerators with Hardware Abstraction},
year = {2022},
isbn = {9781450386104},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3470496.3527440},
doi = {10.1145/3470496.3527440},
abstract = {Hardware specialization is a promising trend to sustain performance growth. Spatial hardware accelerators that employ specialized and hierarchical computation and memory resources have recently shown high performance gains for tensor applications such as deep learning, scientific computing, and data mining. To harness the power of these hardware accelerators, programmers have to use specialized instructions with certain hardware constraints. However, these hardware accelerators and instructions are quite new and there is a lack of understanding of the hardware abstraction, performance optimization space, and automatic methodologies to explore the space. Existing compilers use hand-tuned computation implementations and optimization templates, resulting in sub-optimal performance and heavy development costs.In this paper, we propose AMOS, which is an automatic compilation framework for spatial hardware accelerators. Central to this framework is the hardware abstraction that not only clearly specifies the behavior of spatial hardware instructions, but also formally defines the mapping problem from software to hardware. Based on the abstraction, we develop algorithms and performance models to explore various mappings automatically. Finally, we build a compilation framework that uses the hardware abstraction as compiler intermediate representation (IR), explores both compute mappings and memory mappings, and generates high-performance code for different hardware backends. Our experiments show that AMOS achieves more than 2.50\texttimes{} speedup to hand-optimized libraries on Tensor Core, 1.37\texttimes{} speedup to TVM on vector units of Intel CPU for AVX-512, and up to 25.04\texttimes{} speedup to AutoTVM on dot units of Mali GPU. The source code of AMOS is publicly available.},
booktitle = {Proceedings of the 49th Annual International Symposium on Computer Architecture},
pages = {874â€“887},
numpages = {14},
keywords = {tensor computations, code generation, spatial accelerators, mapping},
location = {New York, New York},
series = {ISCA '22}
}